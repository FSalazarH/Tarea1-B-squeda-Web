{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "<center>\n",
    "    <h1> INF335 - Tecnologías de Búsqueda Web   </h1>\n",
    "    <h2> Tarea 1 </h2>\n",
    "    <h3> Universidad Técnica Federico Santa Maria </h3>\n",
    "    \n",
    "</center>\n",
    "\n",
    "_Marzo 2017_\n",
    "<p>Profesor: Marcelo Mendoza</p>\n",
    " <p>Ayudante: Daniel Rivera</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Objetivos </h2>\n",
    "<ul>\n",
    "<li  style =\"margin: 12px 0px; font-size:16px\"> Implementar y analizar la herramienta de python NLTK (Natural Language Tookit) para trabajar con procesamiento de texto y lenguaje natural. </li>\n",
    "<li style =\"margin: 12px 0px;font-size:16px \" > Estudiar e implementar las estructuras de datos adecuadas para representar un corpus, documentos y palabras con su categorización correspondiente.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Dataset : Amazon Fine Food Review</h2>\n",
    "\n",
    "<p style=\"font-size:16px\"> Para esta tarea se va a trabajar con el dataset de <i>“Amazon Fine Food Review”</i> el cual contiene más de 500.000 críticas de platos de comida y restaurants provenientes de Amazon. El archivo consiste en un .csv (“Comma Separate Values”) el cual contiene la siguiente estructura:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<ol>\n",
    "<li style =\"margin: 5px 0px;\"> <strong>Id</strong> - Id único de cada reseña</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong> ProductId</strong> - Id único que identifica el producto a analizar</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong>UserId</strong> - Id único que identifica al usuario</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>ProfileName</strong> - Nombre del usuario que realizó la reseña</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong>HelpfulnessNumerator</strong> -  número de usuarios que indicaron que encontraron esta crítica util</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>HelpfulnessDenominator</strong> número de usuarios que indicaron que encontraron esta crítica util -</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>Score</strong> - Rating, con valores entre 1 y 5 estrellas</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>Time</strong> - timestamp for the review</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>Summary</strong> - breve resumen de la reseña</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong>Text</strong> - string que contiene la reseña </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy, scipy, nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('amazon-fine-foods/Reviews.csv', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    reseña = []\n",
    "    for row in reader:\n",
    "        reseña.append(row['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocesamiento:</h2>\n",
    "\n",
    "### 1 Si observa el corpus, se dará cuenta de que hay etiquetas <i>html</i> embebidas en algunas reseñas. Para eliminar estas etiquetas , use la libreria <i>Beautiful Soup</i> (link: https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "reseñas = []\n",
    "for i in range(len(reseña)):\n",
    "    markup = reseña[i]\n",
    "    soup = BeautifulSoup(markup, 'lxml')\n",
    "    \n",
    "    #Aprovechamos de convertirlo a minuscula:\n",
    "    reseña_sintag = soup.get_text().lower()\n",
    "    reseñas.append(reseña_sintag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reseñas[524000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A continuación, guardaremos el estado del corpus utilizando la librería Pickle </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using Pickle for save de state of corpus\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"list.pickle\", \"wb\")\n",
    "pickle.dump(reseñas, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"list.pickle\", \"rb\")\n",
    "\n",
    "reseñas = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luego, procederemos a dejar el corpus como una lista de sentencias </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtro = ' ,.: \\\" \\' ()!#$%&/?¿¡<>_-*+' #Filtro para eliminar simbolos no deseados\n",
    "\n",
    "\n",
    "reseña_token = []\n",
    "\n",
    "\n",
    "for i in range(len(reseñas)):\n",
    "    Tokens = reseñas[i].split(' ')\n",
    "    for j in Tokens:\n",
    "    \n",
    "        #Separa en caso de ser palabras con coma:\n",
    "        l = j.split(\",\")  \n",
    "        if(len(l)>1):\n",
    "            for k in l:\n",
    "                reseña_token.append(k.strip(filtro)) \n",
    "                \n",
    "        else:\n",
    "            reseña_token.append(j.strip(filtro))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Convierta el corpus , de modo que solo existan minúsculas (<code>lowercase</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(reseña_token)):\n",
    "    reseña_token[i] = reseña_token[i].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3  Usando la lista de stopwords ortorgada por nltk, elimine aquellas palabras que sean clasificadas como stopwords, es decir, aquellas palabras que poseen poco contexto léxico y no otorgan información relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop.append(\"\")\n",
    "stop.append(\",\")\n",
    "stop.append(\" \")\n",
    "stop.append(\"-\")\n",
    "\n",
    "\n",
    "reseña_token2 = []\n",
    "for i in reseña_token:\n",
    "    if(i not in stop):\n",
    "        reseña_token2.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Elimine las palabras que aparezcan en el corpus con una frecuencia inferior a un umbral definido (ejemplo : inferior a 3) ( para ello, es recomendable determinar previo la frecuencia de cada término usando un diccionario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Creando Diccionario con cada palabra:\n",
    "reseña_token3 = []\n",
    "Dict_reseña = {}\n",
    "for i in set(reseña_token2):\n",
    "    Dict_reseña[i] = 0\n",
    "\n",
    "for i in reseña_token2:\n",
    "    Dict_reseña[i] += 1\n",
    "\n",
    "    \n",
    "##Eliminando la palabra si su frecuencia es menor a 3:\n",
    "for i in reseña_token2:\n",
    "    if(Dict_reseña[i] >= 3 and Dict_reseña[i] != \"\"): #tambien elimina los espacios vacios\n",
    "        reseña_token3.append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Usando nltk, determine los Top-30 collocations mas relevantes del corpus, usando Bigramas .Implemente la función <code>BigramAssocMeasures()</code> y <code>BigramCollocationFinder.from_words()</code>. Recuerde que para este punto el corpus debe estar tokenizado. (mirar documentación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('*glass', 'shards*'),\n",
       " ('*knock', 'wood*'),\n",
       " ('0%fuji', 'apple:serving'),\n",
       " ('0%strawberry', 'banana:serving'),\n",
       " ('0.09%total', 'micro-organisms*'),\n",
       " ('0.40%glucosamine', 'hydrochloride*'),\n",
       " ('022313', '022413'),\n",
       " ('022413', '032813'),\n",
       " ('041113', '052913'),\n",
       " ('042013', '052213'),\n",
       " ('050913', '051513'),\n",
       " ('051813', '051913'),\n",
       " ('07/mar', '12..........so'),\n",
       " ('0;sodium', '15mg;total'),\n",
       " ('0ther', 'additives.oh'),\n",
       " ('1.5cups', 'milk/banana/whey'),\n",
       " ('1.5gfiber', '3gsugar'),\n",
       " ('1/8oz', '1dram'),\n",
       " ('10/14/10...arrived', '10/18/10...expiration'),\n",
       " ('11.27', 'cents.since'),\n",
       " ('120112', '022813'),\n",
       " ('120;fat', '0;sodium'),\n",
       " ('12jul12', '090227921:37'),\n",
       " ('13.90(for', 'members)and'),\n",
       " ('18.57', 'pound....imagine'),\n",
       " ('19current', 'price-$24'),\n",
       " ('2.40%acids', '*omega'),\n",
       " ('2.5.combine', 'nlp'),\n",
       " ('2.5carb', '23.25'),\n",
       " ('2011dvm', 'newsmagazinerockville')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obteniendo top 30 bigrams\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(reseña_token3)\n",
    "finder.nbest(bigram_measures.pmi, 30)  # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.- Usando la libreria incorporada en nltk, implemente Stanford POS tagger para categorizar y obtener los tags de cada token del corpus usando Part-Of-Speech Tagger (POSTagger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.- Usando la libreria incorporada en nltk, implemente Named Entity Recognition (NER) con Stanford NER Tagger. Analice y describa sus resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 .-Sentiment Analysis </strong>: Implemente usando la libreria <i>Vader</i> incorporada en nltk para analizar la polaridad del corpus ,determinar cada documento (para ello es necesario re-estructurar el corpus como un array de documentos, o sentencias):</p>\n",
    "<ol>\n",
    "<li> Tokenizar el corpus a nivel de sentencia (recuerde incorporar el preprocesamiento previo).</li>\n",
    "<li> Para cada sentencias (reseña) , implemente Vader para determinar la polaridad.</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Notas</h2>\n",
    "<ul>\n",
    "<li style =\"margin: 12px 0px; font-size: 16px\" >\n",
    "Para varias etapas del preprocesamiento, usará diferentes librerias disponibles en Python. Se recomienda usar el instalador de paquetes <i>pip</i> ( link: https://pypi.python.org/pypi/pip ) .\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px; font-size: 16px\" >\n",
    "Algunos de estos pasos del preprocesamiento pueden demorar en compilar (en algunos casos sobre 45 min, dependiendo de la máquina), por lo que es recomendable ir guardando el estado del corpus su posterior uso. Para estos casos se recomienda usar la libreria <code> pickle </code> en python (link: https://docs.python.org/2/library/pickle.html ) \n",
    "</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Documentación y Ejemplos</h2>\n",
    "<ul>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Beautiful Soup :\n",
    "https://www.crummy.com/software/BeautifulSoup/\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Bigrams and Collocations:\n",
    "http://www.nltk.org/howto/collocations.html\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Stanford PoS Tagger :\n",
    "http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford\n",
    "</li>\n",
    "<li>\n",
    "Stanford 'Tagger' Link Download : https://nlp.stanford.edu/software/tagger.shtml#Download\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Stanford Ner Tagger:\n",
    "https://pythonprogramming.net/named-entity-recognition-stanford-ner-tagger/\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Sentiment Analysis with Vader: \n",
    "http://www.nltk.org/howto/sentiment.html\n",
    "</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Instrucciones</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol >\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > El informe debe entregarse en un archivo jupyter notebook (diferente a este) con el código  implementado y los análisis correspondientes. El informe debe subirse en la plataforma oficial de moodle en formato comprimido (.zip) con el nombre <i>tarea1_rol.zip</i></p> </li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > Todas las consultas serán atendidas por el canal de consultas de moodle. </i></p> </li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > La fecha de entrega es el dia <strong>10 de Abril</strong> . Pasada esa fecha se descontaran 20 puntos por dia. </p> </li>\n",
    "\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
