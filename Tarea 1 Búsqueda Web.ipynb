{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "<center>\n",
    "    <h1> INF335 - Tecnologías de Búsqueda Web   </h1>\n",
    "    <h2> Tarea 1 </h2>\n",
    "    <h3> Universidad Técnica Federico Santa Maria </h3>\n",
    "    \n",
    "</center>\n",
    "\n",
    "_Marzo 2017_\n",
    "<p>Profesor: Marcelo Mendoza</p>\n",
    " <p>Ayudante: Daniel Rivera</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Objetivos </h2>\n",
    "<ul>\n",
    "<li  style =\"margin: 12px 0px; font-size:16px\"> Implementar y analizar la herramienta de python NLTK (Natural Language Tookit) para trabajar con procesamiento de texto y lenguaje natural. </li>\n",
    "<li style =\"margin: 12px 0px;font-size:16px \" > Estudiar e implementar las estructuras de datos adecuadas para representar un corpus, documentos y palabras con su categorización correspondiente.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Dataset : Amazon Fine Food Review</h2>\n",
    "\n",
    "<p style=\"font-size:16px\"> Para esta tarea se va a trabajar con el dataset de <i>“Amazon Fine Food Review”</i> el cual contiene más de 500.000 críticas de platos de comida y restaurants provenientes de Amazon. El archivo consiste en un .csv (“Comma Separate Values”) el cual contiene la siguiente estructura:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<ol>\n",
    "<li style =\"margin: 5px 0px;\"> <strong>Id</strong> - Id único de cada reseña</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong> ProductId</strong> - Id único que identifica el producto a analizar</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong>UserId</strong> - Id único que identifica al usuario</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>ProfileName</strong> - Nombre del usuario que realizó la reseña</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong>HelpfulnessNumerator</strong> -  número de usuarios que indicaron que encontraron esta crítica util</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>HelpfulnessDenominator</strong> número de usuarios que indicaron que encontraron esta crítica util -</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>Score</strong> - Rating, con valores entre 1 y 5 estrellas</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>Time</strong> - timestamp for the review</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>Summary</strong> - breve resumen de la reseña</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong>Text</strong> - string que contiene la reseña </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px\" > <strong> Link Descarga Dataset: https://drive.google.com/open?id=0B1GNvIDVzwwLR2dwQVliRnBWMnM </strong>  </p> \n",
    "\n",
    "<p style=\"font-size:16px\" > <b> Objetivo: </b> Extraiga del documento el item “Text” y generé un corpus , almacenando en un string todas las reseñas del dataset . Usará esta variable para realizar las siguientes etapas de preprocesamiento de texto.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('amazon-fine-foods/Reviews.csv', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    reseña = []\n",
    "    for row in reader:\n",
    "        reseña.append(row['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reseña[524000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocesamiento:</h2>\n",
    "\n",
    "<ol >\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > Si observa el corpus, se dará cuenta de que hay etiquetas <i>html</i> embebidas en algunas reseñas. Para eliminar estas etiquetas , use la libreria <i>Beautiful Soup</i> (link: https://www.crummy.com/software/BeautifulSoup/bs4/doc/).</p> </li>\n",
    "\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "reseñas = []\n",
    "for i in range(len(reseña)):\n",
    "    markup = reseña[i]\n",
    "    soup = BeautifulSoup(markup, 'lxml')\n",
    "    reseña_sintag = soup.get_text()\n",
    "    reseñas.append(reseña_sintag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reseñas is a list of reseñas of large 568454\n",
    "reseñas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> A continuación, guardaremos el estado del corpus utilizando la librería Pickle </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using Pickle for save de state of corpus\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"list.pickle\", \"wb\")\n",
    "pickle.dump(reseñas, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correr desde acá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_in = open(\"list.pickle\", \"rb\")\n",
    "\n",
    "reseñas = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Luego, procederemos a dejar el corpus como una lista de sentencias </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "reseña_sent_token = []\n",
    "for i in range(len(reseñas)):\n",
    "    #reseña_token.append(reseñas[i].split(' '))\n",
    "    reseña_sent_token.append(sent_tokenize(reseñas[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have bought several of the Vitality canned dog food products and have found them all to be of good quality.',\n",
       " 'The product looks more like a stew than a processed meat and it smells better.',\n",
       " 'My Labrador is finicky and she appreciates this product better than  most.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reseña_sent_token is a list of list that contain a tokenize sentences\n",
    "len(reseña_sent_token)\n",
    "reseña_sent_token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\"> Convierta el corpus , de modo que solo existan minúsculas (<code>lowercase</code>). </p></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(len(reseña_sent_token)):\n",
    "    for j in range(len(reseña_sent_token[i])):\n",
    "        corpus.append(reseña_token[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2839271"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Corpus is a list with all words of our sentences, not a list of list. \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_word_tokenize = []\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    corpus_word_tokenize.append(word_tokenize(corpus[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"'ve\",\n",
       " 'noticed',\n",
       " 'similar',\n",
       " 'reviews',\n",
       " 'related',\n",
       " 'to',\n",
       " 'formula',\n",
       " 'changes',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus_word_tokenize))\n",
    "corpus_word_tokenize[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Usando la lista de stopwords ortorgada por nltk, elimine aquellas palabras que sean clasificadas como stopwords, es decir, aquellas palabras que poseen poco contexto léxico y no otorgan información relevante. </p></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words =set(stopwords.words(\"english\"))\n",
    "stop_words.update(['.', '``', \"''\", '-', '--','...', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) \n",
    "\n",
    "corpus_filtered = []\n",
    "\n",
    "for i in corpus_word_tokenize:\n",
    "    for j in i:\n",
    "        if j not in stop_words:\n",
    "            corpus_filtered.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23702227"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Elimine las palabras que aparezcan en el corpus con una frecuencia inferior a un umbral definido (ejemplo : inferior a 3) ( para ello, es recomendable determinar previo la frecuencia de cada término usando un diccionario). </p></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corpus_filterbyfreq contain the words with frequency > 5 \n",
    "corpus_filterbyfreq = []\n",
    "\n",
    "#diccionario de palabras\n",
    "diccionario = {}\n",
    "\n",
    "for i in set(corpus_filtered):\n",
    "    diccionario[i] = 0\n",
    "\n",
    "for i in corpus_filtered:\n",
    "    diccionario[i]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in corpus_filtered:\n",
    "    if(diccionario[i]>5):\n",
    "        corpus_filterbyfreq.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116631409"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_filterbyfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Usando nltk, determine los Top-30 collocations mas relevantes del corpus, usando Bigramas .Implemente la función <code>BigramAssocMeasures()</code> y <code>BigramCollocationFinder.from_words()</code>. Recuerde que para este punto el corpus debe estar tokenizado. (mirar documentación). </p></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Usando la libreria incorporada en nltk, implemente Stanford POS tagger para categorizar y obtener los tags de cada token del corpus usando Part-Of-Speech Tagger (POSTagger). </p></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Usando la libreria incorporada en nltk, implemente Named Entity Recognition (NER) con Stanford NER Tagger. Analice y describa sus resultados. </p></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > <strong> Sentiment Analysis </strong>: Implemente usando la libreria <i>Vader</i> incorporada en nltk para analizar la polaridad del corpus ,determinar cada documento (para ello es necesario re-estructurar el corpus como un array de documentos, o sentencias):</p>\n",
    "<ol>\n",
    "<li> Tokenizar el corpus a nivel de sentencia (recuerde incorporar el preprocesamiento previo).</li>\n",
    "<li> Para cada sentencias (reseña) , implemente Vader para determinar la polaridad.</li>\n",
    "</ol>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Notas</h2>\n",
    "<ul>\n",
    "<li style =\"margin: 12px 0px; font-size: 16px\" >\n",
    "Para varias etapas del preprocesamiento, usará diferentes librerias disponibles en Python. Se recomienda usar el instalador de paquetes <i>pip</i> ( link: https://pypi.python.org/pypi/pip ) .\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px; font-size: 16px\" >\n",
    "Algunos de estos pasos del preprocesamiento pueden demorar en compilar (en algunos casos sobre 45 min, dependiendo de la máquina), por lo que es recomendable ir guardando el estado del corpus su posterior uso. Para estos casos se recomienda usar la libreria <code> pickle </code> en python (link: https://docs.python.org/2/library/pickle.html ) \n",
    "</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Documentación y Ejemplos</h2>\n",
    "<ul>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Beautiful Soup :\n",
    "https://www.crummy.com/software/BeautifulSoup/\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Bigrams and Collocations:\n",
    "http://www.nltk.org/howto/collocations.html\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Stanford PoS Tagger :\n",
    "http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford\n",
    "</li>\n",
    "<li>\n",
    "Stanford 'Tagger' Link Download : https://nlp.stanford.edu/software/tagger.shtml#Download\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Stanford Ner Tagger:\n",
    "https://pythonprogramming.net/named-entity-recognition-stanford-ner-tagger/\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Sentiment Analysis with Vader: \n",
    "http://www.nltk.org/howto/sentiment.html\n",
    "</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Instrucciones</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol >\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > El informe debe entregarse en un archivo jupyter notebook (diferente a este) con el código  implementado y los análisis correspondientes. El informe debe subirse en la plataforma oficial de moodle en formato comprimido (.zip) con el nombre <i>tarea1_rol.zip</i></p> </li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > Todas las consultas serán atendidas por el canal de consultas de moodle. </i></p> </li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > La fecha de entrega es el dia <strong>10 de Abril</strong> . Pasada esa fecha se descontaran 20 puntos por dia. </p> </li>\n",
    "\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
